{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFKtVVpJJfCf",
        "outputId": "3c23854b-ee94-4304-c76a-75470810e476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812363 sha256=5a8f6cd9914ad1588b49ff40232ad39f2a2ab31c23cfba1d056d234505761370\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1: Data Ingestion - Reading Data from Various Formats"
      ],
      "metadata": {
        "id": "mXvqzFzNKFHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n",
        "\n",
        "# CSV Data\n",
        "csv_data = [(\"S001\", \"Anil Kumar\", 10, 85),\n",
        "            (\"S002\", \"Neha Sharma\", 12, 92),\n",
        "            (\"S003\", \"Rajesh Gupta\", 11, 78)]\n",
        "csv_columns = [\"StudentID\", \"Name\", \"Class\", \"Score\"]\n",
        "csv_df = spark.createDataFrame(csv_data, schema=csv_columns)\n",
        "\n",
        "# JSON Data\n",
        "json_data = [\n",
        "    {\"CityID\": \"C001\", \"CityName\": \"Mumbai\", \"Population\": 20411000},\n",
        "    {\"CityID\": \"C002\", \"CityName\": \"Delhi\", \"Population\": 16787941},\n",
        "    {\"CityID\": \"C003\", \"CityName\": \"Bangalore\", \"Population\": 8443675}\n",
        "]\n",
        "json_df = spark.read.json(spark.sparkContext.parallelize(json_data))\n",
        "\n",
        "# Parquet Data\n",
        "parquet_path = \"/content/sample_data/hospital_data.parquet\"\n",
        "try:\n",
        "    hospital_parquet_df = spark.read.parquet(parquet_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Parquet data: {e}\")\n",
        "\n",
        "# Delta Table\n",
        "delta_table_path = \"/content/sample_data/delta/hospital_records\"\n",
        "try:\n",
        "    hospital_delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error reading Delta table: {e}\")\n"
      ],
      "metadata": {
        "id": "7UFr0gAJJ_eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E3BdhHClKO95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Writing Data to Various Formats"
      ],
      "metadata": {
        "id": "cGuB_6lZMrkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write CSV\n",
        "csv_output_path = \"/content/sample_data/output/students.csv\"\n",
        "csv_df.write.mode(\"overwrite\").csv(csv_output_path)\n",
        "\n",
        "# Write JSON\n",
        "json_output_path = \"/content/sample_data/output/cities.json\"\n",
        "json_df.write.mode(\"overwrite\").json(json_output_path)\n",
        "\n",
        "# Write Parquet\n",
        "parquet_output_path = \"/content/sample_data/output/hospital_data.parquet\"\n",
        "hospital_parquet_df.write.mode(\"overwrite\").parquet(parquet_output_path)\n",
        "\n",
        "# Write Delta Table\n",
        "delta_output_path = \"/content/sample_data/delta/hospital_data\"\n",
        "hospital_parquet_df.write.format(\"delta\").mode(\"overwrite\").save(delta_output_path)"
      ],
      "metadata": {
        "id": "7JO_BTBAKWcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Running One Notebook from Another\n",
        "\n",
        "Notebook 1: Data Ingestion and Cleaning"
      ],
      "metadata": {
        "id": "22qASHEYKave"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"StudentDataIngestion\").getOrCreate()\n",
        "\n",
        "# Ingest data from CSV\n",
        "raw_students_df = spark.read.csv(\"/content/sample_data/students.csv\", header=True)\n",
        "\n",
        "# Clean the data\n",
        "cleaned_students_df = raw_students_df.dropDuplicates().na.fill({\"Score\": 0})\n",
        "\n",
        "# Save as Delta table\n",
        "cleaned_delta_path = \"/content/sample_data/delta/cleaned_students\"\n",
        "cleaned_students_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_path)\n",
        "\n",
        "# Run Notebook B\n",
        "dbutils.notebook.run(\"/content/sample_data/Various_Formats_Notebook_B\", 60)"
      ],
      "metadata": {
        "id": "7F0cqy3sKfXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Databricks Ingestion"
      ],
      "metadata": {
        "id": "0Qas4XhRKqFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read from various sources\n",
        "csv_azure_path = \"abfss://<your-container>@<your-account>.dfs.core.windows.net/<path>/students.csv\"\n",
        "json_dbfs_path = \"/FileStore/cities.json\"\n",
        "parquet_s3_path = \"s3a://<your-bucket>/hospital_data.parquet\"\n",
        "delta_db_path = \"delta.`/content/sample_data/delta/hospital_data`\"\n",
        "\n",
        "# Load data\n",
        "students_df = spark.read.csv(csv_azure_path, header=True)\n",
        "cities_df = spark.read.json(json_dbfs_path)\n",
        "hospital_parquet_df = spark.read.parquet(parquet_s3_path)\n",
        "hospital_delta_df = spark.read.format(\"delta\").load(delta_db_path)\n",
        "\n",
        "# Perform transformations (e.g., filter and calculate totals)\n",
        "transformed_students_df = students_df.filter(col(\"Score\") > 80)\n",
        "total_students = transformed_students_df.count()\n",
        "\n",
        "# Write cleaned data to various formats\n",
        "transformed_students_df.write.mode(\"overwrite\").csv(\"/content/sample_data/output/cleaned_students.csv\")\n",
        "transformed_students_df.write.mode(\"overwrite\").json(\"/content/sample_data/output/cleaned_students.json\")\n",
        "transformed_students_df.write.mode(\"overwrite\").parquet(\"/content/sample_data/output/cleaned_students.parquet\")\n",
        "transformed_students_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/sample_data/delta/cleaned_students\")"
      ],
      "metadata": {
        "id": "FZqwTcKOKrMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Tasks: Optimization, Z-ordering, and Vacuuming"
      ],
      "metadata": {
        "id": "iiWLWRCWKvEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize Delta table\n",
        "spark.sql(\"OPTIMIZE delta.`/content/sample_data/delta/cleaned_students`\")\n",
        "\n",
        "# Apply Z-ordering on Class column\n",
        "spark.sql(\"OPTIMIZE delta.`/content/sample_data/delta/cleaned_students` ZORDER BY (Class)\")\n",
        "\n",
        "# Vacuum old versions of the Delta table\n",
        "spark.sql(\"VACUUM delta.`/content/sample_data/delta/cleaned_students` RETAIN 0 HOURS\")"
      ],
      "metadata": {
        "id": "mBL6I39OKyLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "3Nd5qNnwLq2r"
      }
    }
  ]
}