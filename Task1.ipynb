{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdee916a-b689-4c1c-a2a1-1761b7e041fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/exer.csv\",\"dbfs:/FileStore/exer.csv\")\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/FileStore/exer.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962d9791-6fbb-429d-bf25-9c90d489e98d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: date (nullable = true)\n |-- Salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# Assuming the CSV file is in your Databricks workspace\n",
    "employee_df = spark.read.csv(\"/FileStore/exer.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first 10 rows\n",
    "employee_df.show(10)\n",
    "\n",
    "# Inspect the schema\n",
    "employee_df.printSchema()\n",
    "\n",
    "# Remove rows with Salary less than 55,000\n",
    "filtered_df = employee_df.filter(F.col(\"Salary\") >= 55000)\n",
    "\n",
    "# Filter employees who joined after 2020\n",
    "filtered_df = filtered_df.filter(F.year(F.col(\"JoiningDate\")) > 2020)\n",
    "\n",
    "# Average salary by Department\n",
    "avg_salary_by_department = filtered_df.groupBy(\"Department\").agg(F.avg(\"Salary\").alias(\"AverageSalary\"))\n",
    "\n",
    "# Count of employees in each Department\n",
    "employee_count_by_department = filtered_df.groupBy(\"Department\").count()\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "filtered_df.write.csv(\"cleaned_employee_data.csv\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07a84340-ef29-471c-81fd-07346daed6aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42611b06-73ed-4b6b-b0af-e91edc24db63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\nroot\n |-- Category: string (nullable = true)\n |-- Price: long (nullable = true)\n |-- ProductID: long (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Stock: long (nullable = true)\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|Electronics|  300|      104|    Monitor|   45|\n+-----------+-----+---------+-----------+-----+\n\n+---------+----------+\n| Category|TotalStock|\n+---------+----------+\n|Furniture|        85|\n+---------+----------+\n\n+-----------+-----------------+\n|   Category|         AvgPrice|\n+-----------+-----------------+\n|Electronics|766.6666666666666|\n|  Furniture|            250.0|\n+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/product_data (1).json\", \"dbfs:/FileStore/product_data(1).json\")\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data(1).json\")\n",
    "df.show(10)\n",
    "df.printSchema()\n",
    "\n",
    "# Remove rows where Stock is less than 30.\n",
    "# Filter the products that belong to the \"Electronics\" category.\n",
    "df_cleaned_product = df.filter((df['Stock'] >= 30) & (df['Category'] == 'Electronics'))\n",
    "df_cleaned_product.show()\n",
    "\n",
    "# Calculate the total stock for products in the \"Furniture\" category.\n",
    "df_total_furniture_stock = df.filter(df['Category'] == 'Furniture').groupBy('Category').agg({'Stock': 'sum'}).withColumnRenamed('sum(Stock)', 'TotalStock')\n",
    "df_total_furniture_stock.show()\n",
    "\n",
    "# Find the average price of all products in the dataset.\n",
    "df_avg_price = df.groupBy('Category').agg({'Price': 'avg'}).withColumnRenamed('avg(Price)', 'AvgPrice')\n",
    "df_avg_price.show()\n",
    "\n",
    "# Save the cleaned and aggregated data into a new JSON file.\n",
    "df_cleaned_product.coalesce(1).write.json('/FileStore/cleaned_product_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92b07fe3-28ce-4ece-a2ec-5ba3fe9b2d49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868c65f6-e267-4012-87e5-b052f3c110df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\nroot\n |-- EmployeeID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- JoiningDate: date (nullable = true)\n |-- Salary: integer (nullable = true)\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\nroot\n |-- Category: string (nullable = true)\n |-- Price: long (nullable = true)\n |-- ProductID: long (nullable = true)\n |-- ProductName: string (nullable = true)\n |-- Stock: long (nullable = true)\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics| 1200|      101|     Laptop|   35|\n|Electronics|  800|      102| Smartphone|   80|\n|  Furniture|  150|      103| Desk Chair|   60|\n|Electronics|  300|      104|    Monitor|   45|\n|  Furniture|  350|      105|       Desk|   25|\n+-----------+-----+---------+-----------+-----+\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1001|     John Doe|        HR| 2021-01-15| 55000|\n|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n|      1005| David Wilson|        IT| 2021-06-25| 58000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n|      1007| James Miller|        IT| 2019-08-14| 65000|\n|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n+----------+-------------+----------+-----------+------+\n\n+----------+-------------+----------+-----------+------+\n|EmployeeID|         Name|Department|JoiningDate|Salary|\n+----------+-------------+----------+-----------+------+\n|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n+----------+-------------+----------+-----------+------+\n\n+-----------+-----+---------+-----------+-----+\n|   Category|Price|ProductID|ProductName|Stock|\n+-----------+-----+---------+-----------+-----+\n|Electronics|  800|      102| Smartphone|   80|\n+-----------+-----+---------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Load employee.csv file data\n",
    "df_employee = spark.read.csv('/FileStore/exer.csv', header=True, inferSchema=True).cache()\n",
    "df_employee.show()\n",
    "df_employee.printSchema()\n",
    "\n",
    "# Load product_data.json file\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data(1).json\")\n",
    "df.show(10)\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/delta/exer\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/delta/product_data(1)\")\n",
    "\n",
    "\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS employee_delta USING DELTA LOCATION '/dbfs/FileStore/delta/exer'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS product_delta USING DELTA LOCATION '/dbfs/FileStore/delta/product_data(1)'\")\n",
    "\n",
    "\n",
    "# Increase salary by 5% for IT department employees\n",
    "spark.sql(\"UPDATE employee_delta SET Salary = Salary * 1.05 WHERE Department = 'IT'\")\n",
    "# Delete products where stock is less than 40\n",
    "spark.sql(\"DELETE FROM product_delta WHERE Stock < 40\")\n",
    "\n",
    "\n",
    "# Query the product Delta table to show its state before the delete\n",
    "# operation (use time travel).\n",
    "df_product_version_before_delete = spark.sql(\"SELECT * FROM product_delta VERSION AS OF 0\")\n",
    "df_product_version_before_delete.show()\n",
    "# Retrieve the version of the employee Delta table before the salary update.\n",
    "df_employee_version_before_update = spark.sql(\"SELECT * FROM employee_delta VERSION AS OF 0\")\n",
    "df_employee_version_before_update.show()\n",
    "\n",
    "\n",
    "# Query the employee Delta table to find the employees in the Finance department.\n",
    "df_finance_employees = spark.sql(\"SELECT * FROM employee_delta WHERE Department = 'Finance'\")\n",
    "df_finance_employees.show()\n",
    "# Query the product Delta table to find all products in the Electronics category with a price greater than 500.\n",
    "df_expensive_electronics = spark.sql(\"SELECT * FROM product_delta WHERE Category = 'Electronics' AND Price > 500\")\n",
    "df_expensive_electronics.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
