{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OShXFfQ7GNIh",
        "outputId": "c3cfc2fd-2e5c-42b9-f437-d7ff91156761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=0bde2e625d994892c45e115c7cf9135f404b0bb8bea61404d06798ca96af58eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,expr,avg,sum,max\n",
        "\n",
        "spark=SparkSession.builder \\\n",
        "  .appName(\"EcommerceTransactions\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, 101, 'Laptop', 'Electronics', 1000, 1, 10, '2023-08-01'),\n",
        "    (2, 102, 'Smartphone', 'Electronics', 700, 2, 5, '2023-08-01'),\n",
        "    (3, 103, 'Shirt', 'Fashion', 40, 3, 0, '2023-08-02'),\n",
        "    (4, 104, 'Blender', 'Home Appliance', 150, 1, 15, '2023-08-03'),\n",
        "    (5, 101, 'Headphones', 'Electronics', 100, 2, 10, '2023-08-03'),\n",
        "    (6, 105, 'Shoes', 'Fashion', 60, 1, 20, '2023-08-04'),\n",
        "    (7, 106, 'Refrigerator', 'Home Appliance', 800, 1, 25, '2023-08-05'),\n",
        "    (8, 107, 'Book', 'Books', 20, 4, 0, '2023-08-05'),\n",
        "    (9, 108, 'Toaster', 'Home Appliance', 30, 1, 5, '2023-08-06'),\n",
        "    (10, 102, 'Tablet', 'Electronics', 300, 2, 10, '2023-08-06')\n",
        "]\n",
        "\n",
        "columns = ['transaction_id', 'customer_id', 'product', 'category', 'price', 'quantity', 'discount_percentage', 'transaction_date']\n",
        "df=spark.createDataFrame(data,columns)\n",
        "\n",
        "# 1. Calculate the Total Revenue per Category\n",
        "df = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\") * (1 - col(\"discount_percentage\") / 100))\n",
        "total_revenue_per_category = df.groupBy(\"category\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "print(\"Total Revenue per Category:\")\n",
        "total_revenue_per_category.show()\n",
        "\n",
        "# 2. Filter Transactions with a Discount Greater Than 10%\n",
        "discount_gt_10 = df.filter(col(\"discount_percentage\") > 10)\n",
        "print(\"Transactions with a Discount Greater Than 10%:\")\n",
        "discount_gt_10.show()\n",
        "\n",
        "# 3. Find the Most Expensive Product Sold\n",
        "most_expensive_product = df.orderBy(col(\"price\").desc()).limit(1)\n",
        "print(\"Most Expensive Product Sold:\")\n",
        "most_expensive_product.show()\n",
        "\n",
        "# 4. Calculate the Average Quantity of Products Sold per Category\n",
        "avg_quantity_per_category = df.groupBy(\"category\").agg(avg(\"quantity\").alias(\"avg_quantity\"))\n",
        "print(\"Average Quantity of Products Sold per Category:\")\n",
        "avg_quantity_per_category.show()\n",
        "\n",
        "# 5. Identify Customers Who Purchased More Than One Product in a Single Transaction\n",
        "multiple_products = df.filter(col(\"quantity\") > 1)\n",
        "print(\"Customers Who Purchased More Than One Product in a Single Transaction:\")\n",
        "multiple_products.show()\n",
        "\n",
        "# 6. Find the Top 3 Highest Revenue Transactions\n",
        "top_3_transactions = df.orderBy(col(\"revenue\").desc()).limit(3)\n",
        "print(\"Top 3 Highest Revenue Transactions:\")\n",
        "top_3_transactions.show()\n",
        "\n",
        "# 7. Calculate the Total Number of Transactions per Day\n",
        "transactions_per_day = df.groupBy(\"transaction_date\").count().alias(\"total_transactions\")\n",
        "print(\"Total Number of Transactions per Day:\")\n",
        "transactions_per_day.show()\n",
        "\n",
        "# 8. Find the Customer Who Spent the Most Money\n",
        "total_spent_per_customer = df.groupBy(\"customer_id\").agg(sum(\"revenue\").alias(\"total_spent\"))\n",
        "top_spending_customer = total_spent_per_customer.orderBy(col(\"total_spent\").desc()).limit(1)\n",
        "print(\"Customer Who Spent the Most Money:\")\n",
        "top_spending_customer.show()\n",
        "\n",
        "# 9. Calculate the Average Discount Given per Product Category\n",
        "avg_discount_per_category = df.groupBy(\"category\").agg(avg(\"discount_percentage\").alias(\"avg_discount\"))\n",
        "print(\"Average Discount Given per Product Category:\")\n",
        "avg_discount_per_category.show()\n",
        "\n",
        "# 10. Create a New Column for Final Price After Discount\n",
        "df = df.withColumn(\"final_price\", col(\"price\") * (1 - col(\"discount_percentage\") / 100))\n",
        "print(\"New Column for Final Price After Discount:\")\n",
        "df.select(\"product\", \"final_price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR6cWy4FG5sY",
        "outputId": "5eb75fb4-29b6-4d83-c266-99c9fc13120d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue per Category:\n",
            "+--------------+-------------+\n",
            "|      category|total_revenue|\n",
            "+--------------+-------------+\n",
            "|       Fashion|        168.0|\n",
            "|   Electronics|       2950.0|\n",
            "|Home Appliance|        756.0|\n",
            "|         Books|         80.0|\n",
            "+--------------+-------------+\n",
            "\n",
            "Transactions with a Discount Greater Than 10%:\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|  127.5|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|   48.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|  600.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Most Expensive Product Sold:\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|product|   category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|             1|        101| Laptop|Electronics| 1000|       1|                 10|      2023-08-01|  900.0|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Average Quantity of Products Sold per Category:\n",
            "+--------------+------------+\n",
            "|      category|avg_quantity|\n",
            "+--------------+------------+\n",
            "|       Fashion|         2.0|\n",
            "|   Electronics|        1.75|\n",
            "|Home Appliance|         1.0|\n",
            "|         Books|         4.0|\n",
            "+--------------+------------+\n",
            "\n",
            "Customers Who Purchased More Than One Product in a Single Transaction:\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|   product|   category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|Smartphone|Electronics|  700|       2|                  5|      2023-08-01| 1330.0|\n",
            "|             3|        103|     Shirt|    Fashion|   40|       3|                  0|      2023-08-02|  120.0|\n",
            "|             5|        101|Headphones|Electronics|  100|       2|                 10|      2023-08-03|  180.0|\n",
            "|             8|        107|      Book|      Books|   20|       4|                  0|      2023-08-05|   80.0|\n",
            "|            10|        102|    Tablet|Electronics|  300|       2|                 10|      2023-08-06|  540.0|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Top 3 Highest Revenue Transactions:\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01| 1330.0|\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|  900.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|  600.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Total Number of Transactions per Day:\n",
            "+----------------+-----+\n",
            "|transaction_date|count|\n",
            "+----------------+-----+\n",
            "|      2023-08-01|    2|\n",
            "|      2023-08-02|    1|\n",
            "|      2023-08-03|    2|\n",
            "|      2023-08-06|    2|\n",
            "|      2023-08-04|    1|\n",
            "|      2023-08-05|    2|\n",
            "+----------------+-----+\n",
            "\n",
            "Customer Who Spent the Most Money:\n",
            "+-----------+-----------+\n",
            "|customer_id|total_spent|\n",
            "+-----------+-----------+\n",
            "|        102|     1870.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "Average Discount Given per Product Category:\n",
            "+--------------+------------+\n",
            "|      category|avg_discount|\n",
            "+--------------+------------+\n",
            "|       Fashion|        10.0|\n",
            "|   Electronics|        8.75|\n",
            "|Home Appliance|        15.0|\n",
            "|         Books|         0.0|\n",
            "+--------------+------------+\n",
            "\n",
            "New Column for Final Price After Discount:\n",
            "+------------+-----------+\n",
            "|     product|final_price|\n",
            "+------------+-----------+\n",
            "|      Laptop|      900.0|\n",
            "|  Smartphone|      665.0|\n",
            "|       Shirt|       40.0|\n",
            "|     Blender|      127.5|\n",
            "|  Headphones|       90.0|\n",
            "|       Shoes|       48.0|\n",
            "|Refrigerator|      600.0|\n",
            "|        Book|       20.0|\n",
            "|     Toaster|       28.5|\n",
            "|      Tablet|      270.0|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,expr,avg,sum,max,count,when\n",
        "\n",
        "spark=SparkSession.builder \\\n",
        "  .appName(\"BankingTransactions\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, 201, 'Deposit', 5000.0, '2023-09-01'),\n",
        "    (2, 202, 'Withdrawal', 2000.0, '2023-09-01'),\n",
        "    (3, 203, 'Deposit', 3000.0, '2023-09-02'),\n",
        "    (4, 201, 'Withdrawal', 1500.0, '2023-09-02'),\n",
        "    (5, 204, 'Deposit', 10000.0, '2023-09-03'),\n",
        "    (6, 205, 'Withdrawal', 500.0, '2023-09-03'),\n",
        "    (7, 202, 'Deposit', 2500.0, '2023-09-04'),\n",
        "    (8, 206, 'Withdrawal', 700.0, '2023-09-04'),\n",
        "    (9, 203, 'Deposit', 4000.0, '2023-09-05'),\n",
        "    (10, 204, 'Withdrawal', 3000.0, '2023-09-05')\n",
        "]\n",
        "\n",
        "columns = [\"transaction_id\",\"customer_id\",\"transaction_type\",\"amount\",\"transaction_date\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "\n",
        "# 1. Calculate the Total Deposit and Withdrawal Amounts Group the data by transaction_type and calculate the total amounts for both deposits and withdrawals.\n",
        "total_amount=df.groupBy(\"transaction_type\").agg(sum(\"amount\").alias(\"total_amount\"))\n",
        "print(\"Calculate the Total Deposit and Withdrawal Amounts\")\n",
        "total_amount.show()\n",
        "\n",
        "#2. Filter Transactions Greater Than $3,000 Filter the dataset to show only transactions where the amount is greater than $3,000.\n",
        "transactions_above_3000 = df.filter(col(\"amount\")>3000)\n",
        "print(\"Filter Transactions Greater Than $3,000\")\n",
        "transactions_above_3000.show()\n",
        "\n",
        "#3. Find the Largest Deposit Made Identify the transaction with the highest deposit amount.\n",
        "largest_deposit = df.filter(col(\"transaction_type\") == \"Deposit\").agg(max(\"amount\").alias(\"largest_deposit\"))\n",
        "print(\"Find the Largest Deposit Made\")\n",
        "largest_deposit.show()\n",
        "\n",
        "# 4. Calculate the Average Transaction Amount for Each Transaction Type Group the data by transaction_type and calculate the average amount for deposits and withdrawals.\n",
        "avg_transaction_amount=df.groupBy(\"transaction_type\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
        "print(\"Calculate the Average Transaction Amount for Each Transaction Type\")\n",
        "avg_transaction_amount.show()\n",
        "\n",
        "# 5. Find Customers Who Made Both Deposits and Withdrawals Identify customers who have made at least one deposit and one withdrawal.\n",
        "depo_customers=df.filter(col(\"transaction_type\")==\"Deposit\").select(\"customer_id\").distinct()\n",
        "withdraw_customers= df.filter(col(\"transaction_type\")==\"Withdrawal\").select(\"customer_id\").distinct()\n",
        "customers_both = depo_customers.intersect(withdraw_customers)\n",
        "print(\"Find Customers Who Made Both Deposits and Withdrawals\")\n",
        "customers_both.show()\n",
        "\n",
        "# 6. Calculate the Total Amount of Transactions per Day Group the data by transaction_date and calculate the total amount of all transactions for each day.\n",
        "total_amount_per_day = df.groupBy(\"transaction_date\").agg(sum(\"amount\").alias(\"total_amount\"))\n",
        "print(\"Calculate the Total Amount of Transactions per Day\")\n",
        "total_amount_per_day.show()\n",
        "\n",
        "# 7. Find the Customer with the Highest Total Withdrawal Calculate the total amount withdrawn by each customer and identify the customer with the highest total withdrawal.\n",
        "total_withdrawals = df.filter(col(\"transaction_type\")==\"Withdrawal\").groupBy(\"customer_id\").agg(sum(\"amount\").alias(\"total_withdrawal\"))\n",
        "highest_withdrawal_customer = total_withdrawals.orderBy(col(\"total_withdrawal\").desc()).limit(1)\n",
        "print(\"Find the Customer with the Highest Total Withdrawal\")\n",
        "highest_withdrawal_customer.show()\n",
        "\n",
        "# 8. Calculate the Number of Transactions for Each Customer Group the data by customer_id and calculate the total number of transactions made by each customer.\n",
        "transactions_count_per_customer = df.groupBy(\"customer_id\").agg(count(\"transaction_id\").alias(\"transaction_count\"))\n",
        "print(\"Calculate the Number of Transactions for Each Customer\")\n",
        "transactions_count_per_customer.show()\n",
        "\n",
        "# 9. Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000 Filter the data to show all transactions that occurred on the same day as a withdrawal of more than $1,000.\n",
        "withdrawal_days = df.filter((col(\"transaction_type\") == \"Withdrawal\") & (col(\"amount\") > 1000)).select(\"transaction_date\").distinct()\n",
        "transactions_on_withdrawal_days = df.join(withdrawal_days, \"transaction_date\", \"inner\")\n",
        "print(\"Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000\")\n",
        "transactions_on_withdrawal_days.show()\n",
        "\n",
        "# 10. Create a New Column to Classify Transactions as \"High\" or \"Low\" Value Add a new column transaction_value that classifies a transaction as \"High\" if the amount is greater than $5,000, otherwise classify it as \"Low.\"\n",
        "df_with_value = df.withColumn(\"transaction_value\", when(col(\"amount\") > 5000, \"High\").otherwise(\"Low\"))\n",
        "print(\"Create a New Column to Classify Transactions as 'High' or 'Low' Value\")\n",
        "df_with_value.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6VLd3EmQEKf",
        "outputId": "866a70ea-127e-4748-9e1f-d7eeca1e2079"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------------+-------+----------------+\n",
            "|transaction_id|customer_id|transaction_type| amount|transaction_date|\n",
            "+--------------+-----------+----------------+-------+----------------+\n",
            "|             1|        201|         Deposit| 5000.0|      2023-09-01|\n",
            "|             2|        202|      Withdrawal| 2000.0|      2023-09-01|\n",
            "|             3|        203|         Deposit| 3000.0|      2023-09-02|\n",
            "|             4|        201|      Withdrawal| 1500.0|      2023-09-02|\n",
            "|             5|        204|         Deposit|10000.0|      2023-09-03|\n",
            "|             6|        205|      Withdrawal|  500.0|      2023-09-03|\n",
            "|             7|        202|         Deposit| 2500.0|      2023-09-04|\n",
            "|             8|        206|      Withdrawal|  700.0|      2023-09-04|\n",
            "|             9|        203|         Deposit| 4000.0|      2023-09-05|\n",
            "|            10|        204|      Withdrawal| 3000.0|      2023-09-05|\n",
            "+--------------+-----------+----------------+-------+----------------+\n",
            "\n",
            "Calculate the Total Deposit and Withdrawal Amounts\n",
            "+----------------+------------+\n",
            "|transaction_type|total_amount|\n",
            "+----------------+------------+\n",
            "|         Deposit|     24500.0|\n",
            "|      Withdrawal|      7700.0|\n",
            "+----------------+------------+\n",
            "\n",
            "Filter Transactions Greater Than $3,000\n",
            "+--------------+-----------+----------------+-------+----------------+\n",
            "|transaction_id|customer_id|transaction_type| amount|transaction_date|\n",
            "+--------------+-----------+----------------+-------+----------------+\n",
            "|             1|        201|         Deposit| 5000.0|      2023-09-01|\n",
            "|             5|        204|         Deposit|10000.0|      2023-09-03|\n",
            "|             9|        203|         Deposit| 4000.0|      2023-09-05|\n",
            "+--------------+-----------+----------------+-------+----------------+\n",
            "\n",
            "Find the Largest Deposit Made\n",
            "+---------------+\n",
            "|largest_deposit|\n",
            "+---------------+\n",
            "|        10000.0|\n",
            "+---------------+\n",
            "\n",
            "Calculate the Average Transaction Amount for Each Transaction Type\n",
            "+----------------+--------------+\n",
            "|transaction_type|average_amount|\n",
            "+----------------+--------------+\n",
            "|         Deposit|        4900.0|\n",
            "|      Withdrawal|        1540.0|\n",
            "+----------------+--------------+\n",
            "\n",
            "Find Customers Who Made Both Deposits and Withdrawals\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|        202|\n",
            "|        201|\n",
            "|        204|\n",
            "+-----------+\n",
            "\n",
            "Calculate the Total Amount of Transactions per Day\n",
            "+----------------+------------+\n",
            "|transaction_date|total_amount|\n",
            "+----------------+------------+\n",
            "|      2023-09-01|      7000.0|\n",
            "|      2023-09-02|      4500.0|\n",
            "|      2023-09-03|     10500.0|\n",
            "|      2023-09-05|      7000.0|\n",
            "|      2023-09-04|      3200.0|\n",
            "+----------------+------------+\n",
            "\n",
            "Find the Customer with the Highest Total Withdrawal\n",
            "+-----------+----------------+\n",
            "|customer_id|total_withdrawal|\n",
            "+-----------+----------------+\n",
            "|        204|          3000.0|\n",
            "+-----------+----------------+\n",
            "\n",
            "Calculate the Number of Transactions for Each Customer\n",
            "+-----------+-----------------+\n",
            "|customer_id|transaction_count|\n",
            "+-----------+-----------------+\n",
            "|        202|                2|\n",
            "|        201|                2|\n",
            "|        203|                2|\n",
            "|        204|                2|\n",
            "|        205|                1|\n",
            "|        206|                1|\n",
            "+-----------+-----------------+\n",
            "\n",
            "Find All Transactions That Occurred on the Same Day as a Withdrawal Greater Than $1,000\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|transaction_date|transaction_id|customer_id|transaction_type|amount|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|      2023-09-01|             1|        201|         Deposit|5000.0|\n",
            "|      2023-09-01|             2|        202|      Withdrawal|2000.0|\n",
            "|      2023-09-02|             3|        203|         Deposit|3000.0|\n",
            "|      2023-09-02|             4|        201|      Withdrawal|1500.0|\n",
            "|      2023-09-05|             9|        203|         Deposit|4000.0|\n",
            "|      2023-09-05|            10|        204|      Withdrawal|3000.0|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "\n",
            "Create a New Column to Classify Transactions as 'High' or 'Low' Value\n",
            "+--------------+-----------+----------------+-------+----------------+-----------------+\n",
            "|transaction_id|customer_id|transaction_type| amount|transaction_date|transaction_value|\n",
            "+--------------+-----------+----------------+-------+----------------+-----------------+\n",
            "|             1|        201|         Deposit| 5000.0|      2023-09-01|              Low|\n",
            "|             2|        202|      Withdrawal| 2000.0|      2023-09-01|              Low|\n",
            "|             3|        203|         Deposit| 3000.0|      2023-09-02|              Low|\n",
            "|             4|        201|      Withdrawal| 1500.0|      2023-09-02|              Low|\n",
            "|             5|        204|         Deposit|10000.0|      2023-09-03|             High|\n",
            "|             6|        205|      Withdrawal|  500.0|      2023-09-03|              Low|\n",
            "|             7|        202|         Deposit| 2500.0|      2023-09-04|              Low|\n",
            "|             8|        206|      Withdrawal|  700.0|      2023-09-04|              Low|\n",
            "|             9|        203|         Deposit| 4000.0|      2023-09-05|              Low|\n",
            "|            10|        204|      Withdrawal| 3000.0|      2023-09-05|              Low|\n",
            "+--------------+-----------+----------------+-------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg, max, when, countDistinct, count, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"HealthFitnessTracker\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, '2023-09-01', 12000, 500, 7.0, 'Cardio'),\n",
        "    (2, '2023-09-01', 8000, 400, 6.5, 'Strength'),\n",
        "    (3, '2023-09-01', 15000, 650, 8.0, 'Yoga'),\n",
        "    (1, '2023-09-02', 10000, 450, 6.0, 'Cardio'),\n",
        "    (2, '2023-09-02', 9500, 500, 7.0, 'Cardio'),\n",
        "    (3, '2023-09-02', 14000, 600, 7.5, 'Strength'),\n",
        "    (1, '2023-09-03', 13000, 550, 8.0, 'Yoga'),\n",
        "    (2, '2023-09-03', 12000, 520, 6.5, 'Yoga'),\n",
        "    (3, '2023-09-03', 16000, 700, 7.0, 'Cardio')\n",
        "]\n",
        "\n",
        "\n",
        "columns = [\"user_id\", \"date\", \"steps\", \"calories_burned\", \"hours_of_sleep\", \"workout_type\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# 1. Find the Total Steps Taken by Each User\n",
        "total_steps_per_user = df.groupBy(\"user_id\").agg(sum(\"steps\").alias(\"total_steps\"))\n",
        "print(\"Total Steps Taken by Each User\")\n",
        "total_steps_per_user.show()\n",
        "\n",
        "# 2. Filter Days with More Than 10,000 Steps\n",
        "days_above_10000_steps = df.filter(df.steps > 10000)\n",
        "print(\"Days with More Than 10,000 Steps\")\n",
        "days_above_10000_steps.show()\n",
        "\n",
        "# 3. Calculate the Average Calories Burned by Workout Type\n",
        "avg_calories_per_workout = df.groupBy(\"workout_type\").agg(avg(\"calories_burned\").alias(\"average_calories\"))\n",
        "print(\"Average Calories Burned by Workout Type\")\n",
        "avg_calories_per_workout.show()\n",
        "\n",
        "# 4. Identify the Day with the Most Steps for Each User\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(df.steps.desc())\n",
        "most_steps_per_user = df.withColumn(\"rank\", row_number().over(window_spec)).filter(\"rank = 1\").drop(\"rank\")\n",
        "print(\"Day with the Most Steps for Each User\")\n",
        "most_steps_per_user.show()\n",
        "\n",
        "# 5. Find Users Who Burned More Than 600 Calories on Any Day\n",
        "users_above_600_calories = df.filter(df.calories_burned > 600).select(\"user_id\").distinct()\n",
        "print(\"Users Who Burned More Than 600 Calories on Any Day\")\n",
        "users_above_600_calories.show()\n",
        "\n",
        "# 6. Calculate the Average Hours of Sleep per User\n",
        "avg_sleep_per_user = df.groupBy(\"user_id\").agg(avg(\"hours_of_sleep\").alias(\"average_sleep\"))\n",
        "print(\"Average Hours of Sleep per User\")\n",
        "avg_sleep_per_user.show()\n",
        "\n",
        "# 7. Find the Total Calories Burned per Day\n",
        "total_calories_per_day = df.groupBy(\"date\").agg(sum(\"calories_burned\").alias(\"total_calories\"))\n",
        "(\"Total Calories Burned per Day\")\n",
        "total_calories_per_day.show()\n",
        "\n",
        "# 8. Identify Users Who Did Different Types of Workouts\n",
        "workout_types_per_user = df.groupBy(\"user_id\").agg(countDistinct(\"workout_type\").alias(\"workout_types_count\"))\n",
        "users_with_multiple_workouts = workout_types_per_user.filter(workout_types_per_user.workout_types_count > 1)\n",
        "print(\"Users Who Did Different Types of Workouts\")\n",
        "users_with_multiple_workouts.show()\n",
        "\n",
        "# 9. Calculate the Total Number of Workouts per User\n",
        "total_workouts_per_user = df.groupBy(\"user_id\").agg(count(\"workout_type\").alias(\"total_workouts\"))\n",
        "print(\"Total Number of Workouts per User\")\n",
        "total_workouts_per_user.show()\n",
        "\n",
        "# 10. Create a New Column for \"Active\" Days\n",
        "df_with_active_day = df.withColumn(\"active_day\", when(df.steps > 10000, \"Active\").otherwise(\"Inactive\"))\n",
        "print(\"New Column for 'Active' Days\")\n",
        "df_with_active_day.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwf6oFugSxhq",
        "outputId": "2ffbbc94-e0e3-44b1-a315-ba1086ca6801"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Steps Taken by Each User\n",
            "+-------+-----------+\n",
            "|user_id|total_steps|\n",
            "+-------+-----------+\n",
            "|      1|      35000|\n",
            "|      3|      45000|\n",
            "|      2|      29500|\n",
            "+-------+-----------+\n",
            "\n",
            "Days with More Than 10,000 Steps\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "Average Calories Burned by Workout Type\n",
            "+------------+-----------------+\n",
            "|workout_type| average_calories|\n",
            "+------------+-----------------+\n",
            "|    Strength|            500.0|\n",
            "|        Yoga|573.3333333333334|\n",
            "|      Cardio|            537.5|\n",
            "+------------+-----------------+\n",
            "\n",
            "Day with the Most Steps for Each User\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "Users Who Burned More Than 600 Calories on Any Day\n",
            "+-------+\n",
            "|user_id|\n",
            "+-------+\n",
            "|      3|\n",
            "+-------+\n",
            "\n",
            "Average Hours of Sleep per User\n",
            "+-------+-----------------+\n",
            "|user_id|    average_sleep|\n",
            "+-------+-----------------+\n",
            "|      1|              7.0|\n",
            "|      3|              7.5|\n",
            "|      2|6.666666666666667|\n",
            "+-------+-----------------+\n",
            "\n",
            "+----------+--------------+\n",
            "|      date|total_calories|\n",
            "+----------+--------------+\n",
            "|2023-09-01|          1550|\n",
            "|2023-09-02|          1550|\n",
            "|2023-09-03|          1770|\n",
            "+----------+--------------+\n",
            "\n",
            "Users Who Did Different Types of Workouts\n",
            "+-------+-------------------+\n",
            "|user_id|workout_types_count|\n",
            "+-------+-------------------+\n",
            "|      1|                  2|\n",
            "|      3|                  3|\n",
            "|      2|                  3|\n",
            "+-------+-------------------+\n",
            "\n",
            "Total Number of Workouts per User\n",
            "+-------+--------------+\n",
            "|user_id|total_workouts|\n",
            "+-------+--------------+\n",
            "|      1|             3|\n",
            "|      3|             3|\n",
            "|      2|             3|\n",
            "+-------+--------------+\n",
            "\n",
            "New Column for 'Active' Days\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|active_day|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|    Active|\n",
            "|      2|2023-09-01| 8000|            400|           6.5|    Strength|  Inactive|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|    Active|\n",
            "|      1|2023-09-02|10000|            450|           6.0|      Cardio|  Inactive|\n",
            "|      2|2023-09-02| 9500|            500|           7.0|      Cardio|  Inactive|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|    Active|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|    Active|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|    Active|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|    Active|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum, avg, max, count, row_number, col, to_date, countDistinct, when,desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"MusicStreamingData\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Blinding Lights\", \"The Weeknd\", 200, \"2023-09-01 08:15:00\", \"New York\"),\n",
        "    (2, \"Shape of You\", \"Ed Sheeran\", 240, \"2023-09-01 09:20:00\", \"Los Angeles\"),\n",
        "    (3, \"Levitating\", \"Dua Lipa\", 180, \"2023-09-01 10:30:00\", \"London\"),\n",
        "    (1, \"Starboy\", \"The Weeknd\", 220, \"2023-09-01 11:00:00\", \"New York\"),\n",
        "    (2, \"Perfect\", \"Ed Sheeran\", 250, \"2023-09-01 12:15:00\", \"Los Angeles\"),\n",
        "    (3, \"Don't Start Now\", \"Dua Lipa\", 200, \"2023-09-02 08:10:00\", \"London\"),\n",
        "    (1, \"Save Your Tears\", \"The Weeknd\", 210, \"2023-09-02 09:00:00\", \"New York\"),\n",
        "    (2, \"Galway Girl\", \"Ed Sheeran\", 190, \"2023-09-02 10:00:00\", \"Los Angeles\"),\n",
        "    (3, \"New Rules\", \"Dua Lipa\", 230, \"2023-09-02 11:00:00\", \"London\")\n",
        "]\n",
        "\n",
        "\n",
        "columns = [\"user_id\", \"song_title\", \"artist\", \"duration_seconds\", \"streaming_time\", \"location\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# 1. Calculate the Total Listening Time for Each User\n",
        "total_listening_time = df.groupBy(\"user_id\").agg(sum(\"duration_seconds\").alias(\"total_listening_time\"))\n",
        "print(\"Total Listening Time for Each User\")\n",
        "total_listening_time.show()\n",
        "\n",
        "# 2. Filter Songs Streamed for More Than 200 Seconds\n",
        "songs_more_than_200 = df.filter(col(\"duration_seconds\")> 200)\n",
        "print(\"Songs Streamed for More Than 200 Seconds\")\n",
        "songs_more_than_200.show()\n",
        "\n",
        "# 3. Find the Most Popular Artist (by Total Streams)\n",
        "popular_artist = df.groupBy(\"artist\").agg(count(\"song_title\").alias(\"total_streams\")).orderBy(col(\"total_streams\").desc()).limit(1)\n",
        "print(\"Most Popular Artist\")\n",
        "popular_artist.show()\n",
        "\n",
        "# 4. Identify the Song with the Longest Duration\n",
        "longest_song = df.orderBy(col(\"duration_seconds\").desc()).limit(1)\n",
        "print(\"Song with the Longest Duration\")\n",
        "longest_song.show()\n",
        "\n",
        "# 5. Calculate the Average Song Duration by Artist\n",
        "avg_song_duration_by_artist = df.groupBy(\"artist\").agg(avg(\"duration_seconds\").alias(\"average_duration\"))\n",
        "print(\"Average Song Duration by Artist\")\n",
        "avg_song_duration_by_artist.show()\n",
        "\n",
        "# 6. Find the Top 3 Most Streamed Songs per User\n",
        "song_stream_counts = df.groupBy(\"user_id\", \"song_title\").agg(count(\"*\").alias(\"stream_count\"))\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(desc(\"stream_count\"))\n",
        "ranked_songs = song_stream_counts.withColumn(\"rank\", row_number().over(window_spec))\n",
        "top_3_songs_per_user = ranked_songs.filter(col(\"rank\") <= 3).select(\"user_id\", \"song_title\", \"stream_count\")\n",
        "top_3_songs_per_user.show\n",
        "\n",
        "# 7. Calculate the Total Number of Streams per Day\n",
        "total_streams_per_day = df.withColumn(\"stream_date\", to_date(col(\"streaming_time\"))).groupBy(\"stream_date\").agg(count(\"song_title\").alias(\"total_streams\"))\n",
        "print(\"Total Number of Streams per Day\")\n",
        "total_streams_per_day.show()\n",
        "\n",
        "# 8. Identify Users Who Streamed Songs from More Than One Artist\n",
        "users_multiple_artists = df.groupBy(\"user_id\").agg(countDistinct(\"artist\").alias(\"artist_count\")).filter(col(\"artist_count\") > 1)\n",
        "print(\"Users Who Streamed Songs from More Than One Artist\")\n",
        "users_multiple_artists.show()\n",
        "\n",
        "# 9. Calculate the Total Streams for Each Location\n",
        "total_streams_by_location = df.groupBy(\"location\").agg(count(\"song_title\").alias(\"total_streams\"))\n",
        "print(\"Total Streams for Each Location\")\n",
        "total_streams_by_location.show()\n",
        "\n",
        "# 10. Create a New Column to Classify Long and Short Songs\n",
        "df_with_song_length = df.withColumn(\"song_length\", when(df.duration_seconds > 200, \"Long\").otherwise(\"Short\"))\n",
        "print(\"a New Column to Classify Long and Short Songs\")\n",
        "df_with_song_length.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO9TVFRbmsms",
        "outputId": "90767b97-02da-4279-c830-53bdbb3a8b5e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Listening Time for Each User\n",
            "+-------+--------------------+\n",
            "|user_id|total_listening_time|\n",
            "+-------+--------------------+\n",
            "|      1|                 630|\n",
            "|      3|                 610|\n",
            "|      2|                 680|\n",
            "+-------+--------------------+\n",
            "\n",
            "Songs Streamed for More Than 200 Seconds\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "Most Popular Artist\n",
            "+--------+-------------+\n",
            "|  artist|total_streams|\n",
            "+--------+-------------+\n",
            "|Dua Lipa|            3|\n",
            "+--------+-------------+\n",
            "\n",
            "Song with the Longest Duration\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|user_id|song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "Average Song Duration by Artist\n",
            "+----------+------------------+\n",
            "|    artist|  average_duration|\n",
            "+----------+------------------+\n",
            "|  Dua Lipa|203.33333333333334|\n",
            "|Ed Sheeran|226.66666666666666|\n",
            "|The Weeknd|             210.0|\n",
            "+----------+------------------+\n",
            "\n",
            "Total Number of Streams per Day\n",
            "+-----------+-------------+\n",
            "|stream_date|total_streams|\n",
            "+-----------+-------------+\n",
            "| 2023-09-01|            5|\n",
            "| 2023-09-02|            4|\n",
            "+-----------+-------------+\n",
            "\n",
            "Users Who Streamed Songs from More Than One Artist\n",
            "+-------+------------+\n",
            "|user_id|artist_count|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n",
            "Total Streams for Each Location\n",
            "+-----------+-------------+\n",
            "|   location|total_streams|\n",
            "+-----------+-------------+\n",
            "|Los Angeles|            3|\n",
            "|     London|            3|\n",
            "|   New York|            3|\n",
            "+-----------+-------------+\n",
            "\n",
            "a New Column to Classify Long and Short Songs\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|      Short|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|      Short|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, desc, row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"RetailStoreSalesData\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Apple\", \"Groceries\", 0.50, 10, \"2023-09-01\"),\n",
        "    (2, \"T-shirt\", \"Clothing\", 15.00, 2, \"2023-09-01\"),\n",
        "    (3, \"Notebook\", \"Stationery\", 2.00, 5, \"2023-09-02\"),\n",
        "    (4, \"Banana\", \"Groceries\", 0.30, 12, \"2023-09-02\"),\n",
        "    (5, \"Laptop\", \"Electronics\", 800.00, 1, \"2023-09-03\"),\n",
        "    (6, \"Pants\", \"Clothing\", 25.00, 3, \"2023-09-03\"),\n",
        "    (7, \"Headphones\", \"Electronics\", 100.00, 2, \"2023-09-04\"),\n",
        "    (8, \"Pen\", \"Stationery\", 1.00, 10, \"2023-09-04\"),\n",
        "    (9, \"Orange\", \"Groceries\", 0.60, 8, \"2023-09-05\"),\n",
        "    (10, \"Sneakers\", \"Clothing\", 50.00, 1, \"2023-09-05\")\n",
        "]\n",
        "\n",
        "columns = [\"transaction_id\", \"product_name\", \"category\", \"price\", \"quantity\", \"sales_date\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "\n",
        "df_with_revenue = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
        "\n",
        "# 1. Calculate the Total Revenue per Category\n",
        "total_revenue_per_category = df_with_revenue.groupBy(\"category\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "print(\"Total Revenue per Category\")\n",
        "total_revenue_per_category.show()\n",
        "\n",
        "# 2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n",
        "filtered_transactions = df_with_revenue.filter(col(\"revenue\") > 100)\n",
        "print(\"Transactions Where the Total Sales Amount is Greater Than $100\")\n",
        "filtered_transactions.show()\n",
        "\n",
        "# 3. Find the Most Sold Product\n",
        "total_quantity_per_product = df.groupBy(\"product_name\").agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
        "most_sold_product = total_quantity_per_product.orderBy(desc(\"total_quantity\")).first()\n",
        "print(\"Most Sold Product:\", most_sold_product)\n",
        "\n",
        "# 4. Calculate the Average Price per Product Category\n",
        "average_price_per_category = df.groupBy(\"category\").agg(sum(\"price\").alias(\"total_price\"), sum(\"quantity\").alias(\"total_quantity\"))\n",
        "average_price_per_category = average_price_per_category.withColumn(\"average_price\", col(\"total_price\") / col(\"total_quantity\"))\n",
        "print(\"Average Price per Product Category\")\n",
        "average_price_per_category.select(\"category\", \"average_price\").show()\n",
        "\n",
        "# 5. Find the Top 3 Highest Grossing Products\n",
        "top_3_grossing_products = total_revenue_per_category.orderBy(desc(\"total_revenue\")).limit(3)\n",
        "print(\"Top 3 Highest Grossing Products\")\n",
        "top_3_grossing_products.show()\n",
        "\n",
        "# 6. Calculate the Total Number of Items Sold per Day\n",
        "total_quantity_per_day = df.groupBy(\"sales_date\").agg(sum(\"quantity\").alias(\"total_quantity\"))\n",
        "print(\"Total Number of Items Sold per Day\")\n",
        "total_quantity_per_day.show()\n",
        "\n",
        "# 7. Identify the Product with the Lowest Price in Each Category\n",
        "window_spec = Window.partitionBy(\"category\").orderBy(\"price\")\n",
        "ranked_products = df.withColumn(\"rank\", row_number().over(window_spec))\n",
        "lowest_price_per_category = ranked_products.filter(col(\"rank\") == 1)\n",
        "print(\"Product with the Lowest Price in Each Category\")\n",
        "lowest_price_per_category.show()\n",
        "\n",
        "# 8. Calculate the Total Revenue for Each Product\n",
        "total_revenue_per_product = df_with_revenue.groupBy(\"product_name\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "print(\"Total Revenue for Each Product\")\n",
        "total_revenue_per_product.show()\n",
        "\n",
        "# 9. Find the Total Sales per Day for Each Category\n",
        "total_sales_per_day_category = df_with_revenue.groupBy(\"sales_date\", \"category\").agg(sum(\"revenue\").alias(\"total_sales\"))\n",
        "print(\"Total Sales per Day for Each Category\")\n",
        "total_sales_per_day_category.show()\n",
        "\n",
        "# 10. Create a New Column for Discounted Price\n",
        "df_with_discounted_price = df.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
        "print(\"New Column for Discounted Price\")\n",
        "df_with_discounted_price.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uzrZdKEpGhT",
        "outputId": "2dac9500-b1e8-42f5-c00d-8f190140a299"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue per Category\n",
            "+-----------+------------------+\n",
            "|   category|     total_revenue|\n",
            "+-----------+------------------+\n",
            "| Stationery|              20.0|\n",
            "|  Groceries|13.399999999999999|\n",
            "|Electronics|            1000.0|\n",
            "|   Clothing|             155.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "Transactions Where the Total Sales Amount is Greater Than $100\n",
            "+--------------+------------+-----------+-----+--------+----------+-------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|revenue|\n",
            "+--------------+------------+-----------+-----+--------+----------+-------+\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|  800.0|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|  200.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+-------+\n",
            "\n",
            "Most Sold Product: Row(product_name='Banana', total_quantity=12)\n",
            "Average Price per Product Category\n",
            "+-----------+-------------------+\n",
            "|   category|      average_price|\n",
            "+-----------+-------------------+\n",
            "| Stationery|                0.2|\n",
            "|  Groceries|0.04666666666666666|\n",
            "|Electronics|              300.0|\n",
            "|   Clothing|               15.0|\n",
            "+-----------+-------------------+\n",
            "\n",
            "Top 3 Highest Grossing Products\n",
            "+-----------+-------------+\n",
            "|   category|total_revenue|\n",
            "+-----------+-------------+\n",
            "|Electronics|       1000.0|\n",
            "|   Clothing|        155.0|\n",
            "| Stationery|         20.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "Total Number of Items Sold per Day\n",
            "+----------+--------------+\n",
            "|sales_date|total_quantity|\n",
            "+----------+--------------+\n",
            "|2023-09-01|            12|\n",
            "|2023-09-02|            17|\n",
            "|2023-09-03|             4|\n",
            "|2023-09-05|             9|\n",
            "|2023-09-04|            12|\n",
            "+----------+--------------+\n",
            "\n",
            "Product with the Lowest Price in Each Category\n",
            "+--------------+------------+-----------+-----+--------+----------+----+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|rank|\n",
            "+--------------+------------+-----------+-----+--------+----------+----+\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|   1|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|   1|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|   1|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|   1|\n",
            "+--------------+------------+-----------+-----+--------+----------+----+\n",
            "\n",
            "Total Revenue for Each Product\n",
            "+------------+------------------+\n",
            "|product_name|     total_revenue|\n",
            "+------------+------------------+\n",
            "|     T-shirt|              30.0|\n",
            "|      Banana|3.5999999999999996|\n",
            "|      Laptop|             800.0|\n",
            "|    Notebook|              10.0|\n",
            "|       Apple|               5.0|\n",
            "|    Sneakers|              50.0|\n",
            "|      Orange|               4.8|\n",
            "|         Pen|              10.0|\n",
            "|       Pants|              75.0|\n",
            "|  Headphones|             200.0|\n",
            "+------------+------------------+\n",
            "\n",
            "Total Sales per Day for Each Category\n",
            "+----------+-----------+------------------+\n",
            "|sales_date|   category|       total_sales|\n",
            "+----------+-----------+------------------+\n",
            "|2023-09-01|  Groceries|               5.0|\n",
            "|2023-09-02|  Groceries|3.5999999999999996|\n",
            "|2023-09-01|   Clothing|              30.0|\n",
            "|2023-09-02| Stationery|              10.0|\n",
            "|2023-09-03|Electronics|             800.0|\n",
            "|2023-09-05|  Groceries|               4.8|\n",
            "|2023-09-04| Stationery|              10.0|\n",
            "|2023-09-04|Electronics|             200.0|\n",
            "|2023-09-03|   Clothing|              75.0|\n",
            "|2023-09-05|   Clothing|              50.0|\n",
            "+----------+-----------+------------------+\n",
            "\n",
            "New Column for Discounted Price\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|discounted_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|            0.45|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|            13.5|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|             1.8|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|            0.27|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|           720.0|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|            22.5|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|            90.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|             0.9|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|            0.54|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|            45.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}