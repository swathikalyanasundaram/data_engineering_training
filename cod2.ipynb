{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSZ3InZNhqav",
        "outputId": "39f99a52-279b-40eb-9331-e65c83552add"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=22d761d995e6fcb66a242c47ceb39bdb8420dc85c191d008bd3c2849406af514\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CSV file with movie ratings data\n",
        "csv_data = \"\"\"UserID,MovieID,Rating,Timestamp\n",
        "U001,M001,4,2024-05-01 14:30:00\n",
        "U002,M002,5,2024-05-01 16:00:00\n",
        "U003,M001,3,2024-05-02 10:15:00\n",
        "U001,M003,2,2024-05-02 13:45:00\n",
        "U004,M002,4,2024-05-03 18:30:00\n",
        "\"\"\"\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open('/content/movie_ratings.csv', 'w') as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "print(\"CSV file created at /content/movie_ratings.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjujTwnDiVq2",
        "outputId": "4bd0052c-1bb4-4d33-bffa-a111da399ba5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at /content/movie_ratings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s_F3wMDqhfD5",
        "outputId": "61a186c4-93e5-4896-e74a-b9ce3cc404a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during data ingestion: An error occurred while calling o52.save.\n",
            ": java.lang.AbstractMethodError: Receiver class org.apache.spark.sql.delta.commands.WriteIntoDelta does not define or inherit an implementation of the resolved method 'abstract void org$apache$spark$sql$catalyst$plans$logical$Command$_setter_$nodePatterns_$eq(scala.collection.Seq)' of interface org.apache.spark.sql.catalyst.plans.logical.Command.\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.Command.$init$(Command.scala:38)\n",
            "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.<init>(WriteIntoDelta.scala:53)\n",
            "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:154)\n",
            "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:304)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "import os\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Initialize Spark session with Delta support\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MovieRatingsDataIngestion\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\").getOrCreate()\n",
        "\n",
        "# Define schema for movie ratings data\n",
        "schema = StructType([\n",
        "    StructField(\"UserID\", StringType(), True),\n",
        "    StructField(\"MovieID\", StringType(), True),\n",
        "    StructField(\"Rating\", IntegerType(), True),\n",
        "    StructField(\"Timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Define paths\n",
        "raw_data_path = \"/content/movie_ratings.csv\"\n",
        "delta_table_path = \"/content/movie_ratings_delta\"\n",
        "\n",
        "# Ingest data\n",
        "if os.path.exists(raw_data_path):\n",
        "    try:\n",
        "        # Read the CSV data\n",
        "        ratings_df = spark.read.csv(raw_data_path, schema=schema, header=True)\n",
        "\n",
        "        # Check for invalid ratings\n",
        "        invalid_ratings_df = ratings_df.filter((F.col(\"Rating\") < 1) | (F.col(\"Rating\") > 5))\n",
        "        if invalid_ratings_df.count() > 0:\n",
        "            print(f\"Invalid ratings found:\\n{invalid_ratings_df.show()}\")\n",
        "\n",
        "        # Write to Delta table\n",
        "        ratings_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "        print(\"Data loaded and saved as Delta table.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")\n",
        "else:\n",
        "    print(f\"File not found: {raw_data_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Data Cleaning"
      ],
      "metadata": {
        "id": "EGgSL-S8lKRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing Delta table\n",
        "ratings_df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "\n",
        "# Clean the data\n",
        "cleaned_df = ratings_df.filter((F.col(\"Rating\") >= 1) & (F.col(\"Rating\") <= 5)) \\\n",
        "                        .dropDuplicates([\"UserID\", \"MovieID\"])\n",
        "\n",
        "# Save cleaned data to a new Delta table\n",
        "cleaned_delta_table_path = \"/content/movie_ratings_cleaned_delta\"\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n",
        "print(\"Cleaned data saved to Delta table.\")\n"
      ],
      "metadata": {
        "id": "NdK74b3xlc_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Movie Rating Analysis"
      ],
      "metadata": {
        "id": "fEQTTR8WleAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cleaned data\n",
        "cleaned_df = spark.read.format(\"delta\").load(cleaned_delta_table_path)\n",
        "\n",
        "# Analyze the ratings\n",
        "analysis_df = cleaned_df.groupBy(\"MovieID\").agg(\n",
        "    F.avg(\"Rating\").alias(\"AverageRating\"),\n",
        "    F.count(\"Rating\").alias(\"NumberOfRatings\")\n",
        ")\n",
        "\n",
        "# Identify highest and lowest rated movies\n",
        "highest_rated = analysis_df.orderBy(F.desc(\"AverageRating\")).limit(1)\n",
        "lowest_rated = analysis_df.orderBy(\"AverageRating\").limit(1)\n",
        "\n",
        "# Save analysis results to a Delta table\n",
        "analysis_delta_table_path = \"/content/movie_rating_analysis_delta\"\n",
        "analysis_df.write.format(\"delta\").mode(\"overwrite\").save(analysis_delta_table_path)\n",
        "\n",
        "# Show results\n",
        "highest_rated.show()\n",
        "lowest_rated.show()\n"
      ],
      "metadata": {
        "id": "7WAXPazvlhea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Time Travel and Delta Lake History"
      ],
      "metadata": {
        "id": "KZW-TimPllRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update some ratings for demonstration\n",
        "updated_df = cleaned_df.withColumn(\"Rating\", F.when(F.col(\"UserID\") == \"U001\", 5).otherwise(F.col(\"Rating\")))\n",
        "updated_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "\n",
        "# Rollback to the previous version\n",
        "# Get the current version number\n",
        "current_version = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\").count() - 1\n",
        "\n",
        "# Rollback to the previous version\n",
        "rollback_df = spark.read.format(\"delta\").option(\"versionAsOf\", current_version - 1).load(delta_table_path)\n",
        "\n",
        "# Show the rolled-back ratings\n",
        "rollback_df.show()\n",
        "\n",
        "# Describe history\n",
        "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
        "history_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "PoKk9XXxloCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: Optimize Delta Table"
      ],
      "metadata": {
        "id": "OSqY9omalrxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimize the Delta table with Z-ordering on the MovieID column\n",
        "spark.sql(f\"OPTIMIZE delta.`{delta_table_path}` ZORDER BY (MovieID)\")\n",
        "\n",
        "# Clean up older versions\n",
        "spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 0 HOURS\")\n",
        "print(\"Optimization and cleanup completed.\")\n"
      ],
      "metadata": {
        "id": "fkTPDSPllwQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}