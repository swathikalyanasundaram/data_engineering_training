{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5mm1KunWi4b",
        "outputId": "a230194c-8eb4-4906-f699-21ef8cb17b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=18822fee7b1f7934816731ed09b10dd2654e57e9e57b3c38aa28b212bc4fcb17\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Vehicle Maintenance Data Ingestion"
      ],
      "metadata": {
        "id": "3sKLESrjdOTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import input_file_name\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType\n",
        "import os\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"VehicleMaintenanceDataIngestion\") \\\n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtensions\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema for vehicle maintenance data\n",
        "schema = StructType([\n",
        "    StructField(\"VehicleID\", StringType(), True),\n",
        "    StructField(\"Date\", DateType(), True),\n",
        "    StructField(\"ServiceType\", StringType(), True),\n",
        "    StructField(\"ServiceCost\", FloatType(), True),\n",
        "    StructField(\"Mileage\", FloatType(), True)\n",
        "])\n",
        "\n",
        "# Define path to the raw data\n",
        "raw_data_path = \"/content/vehicle_maintenance.csv\"\n",
        "delta_table_path = \"/content/vehicle_maintenance_delta\"\n",
        "\n",
        "# Check if the CSV file exists\n",
        "if os.path.exists(raw_data_path):\n",
        "    try:\n",
        "        # Read the CSV file into a DataFrame\n",
        "        vehicle_df = spark.read.csv(raw_data_path, schema=schema, header=True) \\\n",
        "            .withColumn(\"file_name\", input_file_name())\n",
        "\n",
        "        # Write to Delta table\n",
        "        vehicle_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "        print(\"Data loaded and saved as Delta table.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data ingestion: {e}\")\n",
        "else:\n",
        "    print(f\"File not found: {raw_data_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq1SQvEAfm8u",
        "outputId": "28c9ac64-d826-4cf5-c0f6-592fde8464d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during data ingestion: An error occurred while calling o223.save.\n",
            ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.lang.ClassNotFoundException: delta.DefaultSource\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
            "\tat scala.util.Failure.orElse(Try.scala:224)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
            "\t... 16 more\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nuHXTNntfpyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Data Cleaning\n"
      ],
      "metadata": {
        "id": "a3a9-vcTdj10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "# Clean the vehicle maintenance data\n",
        "try:\n",
        "    # Read the Delta table into a DataFrame\n",
        "    vehicle_df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "\n",
        "    # Ensure positive values and remove duplicates\n",
        "    cleaned_df = vehicle_df.filter((col(\"ServiceCost\") > 0) & (col(\"Mileage\") > 0)) \\\n",
        "                            .dropDuplicates([\"VehicleID\", \"Date\"])\n",
        "\n",
        "    # Save cleaned data to a new Delta table\n",
        "    cleaned_delta_table_path = \"/content/vehicle_maintenance_cleaned_delta\"\n",
        "    cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(cleaned_delta_table_path)\n",
        "    print(\"Cleaned data saved to new Delta table.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data cleaning: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRoOU_EHfsQ_",
        "outputId": "5eeff91f-2855-434e-e4d9-7c371a7da9ce"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during data cleaning: An error occurred while calling o237.load.\n",
            ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
            "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
            "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.lang.ClassNotFoundException: delta.DefaultSource\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
            "\tat scala.util.Failure.orElse(Try.scala:224)\n",
            "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
            "\t... 15 more\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Vehicle Maintenance Analysis"
      ],
      "metadata": {
        "id": "Hn2rxvmZdtqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Analyze the vehicle maintenance data\n",
        "try:\n",
        "    # Read the cleaned Delta table into a DataFrame\n",
        "    cleaned_df = spark.read.format(\"delta\").load(cleaned_delta_table_path)\n",
        "\n",
        "    # Calculate total maintenance cost for each vehicle\n",
        "    total_cost_df = cleaned_df.groupBy(\"VehicleID\").agg({\"ServiceCost\": \"sum\"}) \\\n",
        "                               .withColumnRenamed(\"sum(ServiceCost)\", \"TotalServiceCost\")\n",
        "\n",
        "    # Identify vehicles exceeding a mileage threshold\n",
        "    mileage_threshold = 30000\n",
        "    exceeding_mileage_df = cleaned_df.filter(col(\"Mileage\") > mileage_threshold)\n",
        "\n",
        "    # Save analysis results to Delta tables\n",
        "    total_cost_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/total_service_cost_delta\")\n",
        "    exceeding_mileage_df.write.format(\"delta\").mode(\"overwrite\").save(\"/content/exceeding_mileage_delta\")\n",
        "    print(\"Analysis results saved to Delta tables.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during analysis: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk8HG9MSf0Cu",
        "outputId": "3f37c1a6-d42e-42d2-df01-33f35cab6170"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during analysis: name 'cleaned_delta_table_path' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Data Governance with Delta Lake"
      ],
      "metadata": {
        "id": "C8UFYJHfdziw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Governance with Delta Lake\n",
        "try:\n",
        "    # Perform VACUUM on the original Delta table\n",
        "    spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 0 HOURS\")\n",
        "\n",
        "    # Describe history of the Delta table\n",
        "    history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
        "    history_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"Error during data governance: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "truhJfmFd3X2",
        "outputId": "b25ef45e-735a-438f-fa15-bd9f3e1b2b14"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during data governance: \n",
            "[PARSE_SYNTAX_ERROR] Syntax error at or near 'VACUUM'.(line 1, pos 0)\n",
            "\n",
            "== SQL ==\n",
            "VACUUM delta.`/content/vehicle_maintenance_delta` RETAIN 0 HOURS\n",
            "^^^\n",
            "\n"
          ]
        }
      ]
    }
  ]
}